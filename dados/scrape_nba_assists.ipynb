{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install requests beautifulsoup4 pandas networkx matplotlib lxml html5lib"
      ],
      "metadata": {
        "id": "35qR6LeuyU-D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o4--IHiOxNRJ"
      },
      "outputs": [],
      "source": [
        "import re, requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "GSW_PLAYERS = {\n",
        "    \"Stephen Curry\",\"Klay Thompson\",\"Kevin Durant\",\"Draymond Green\",\"Zaza Pachulia\",\n",
        "    \"Andre Iguodala\",\"Shaun Livingston\",\"David West\",\"JaVale McGee\",\"Jordan Bell\",\n",
        "    \"Kevon Looney\",\"Omri Casspi\",\"Nick Young\",\"Patrick McCaw\",\"Quinn Cook\"\n",
        "}\n",
        "\n",
        "def box_to_pbp_url(bbr_box_url: str) -> str:\n",
        "    return bbr_box_url.replace(\"/boxscores/\",\"/boxscores/pbp/\")\n",
        "\n",
        "ASSIST_RE = re.compile(r\"\\(assist by ([^)]+)\\)\", re.IGNORECASE)\n",
        "MAKES_RE = re.compile(r\"^(.*?)\\s+makes\\s+\", re.IGNORECASE)\n",
        "\n",
        "def parse_pbp_pairs(pbp_url: str, gsw_players: set):\n",
        "    \"\"\"Retorna lista de (assistant, finisher) para o GSW a partir do BBRef PBP.\"\"\"\n",
        "    html = requests.get(pbp_url, timeout=30).text\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "\n",
        "    desc_cells = soup.select(\"td[class^='font-xxs'] , td[class*='pbp'] , td[class*='descr'] , td:nth-of-type(6)\")\n",
        "    if not desc_cells:\n",
        "        desc_cells = soup.find_all(\"td\")\n",
        "\n",
        "    pairs = []\n",
        "    for td in desc_cells:\n",
        "        text = \" \".join(td.get_text(\" \").split())\n",
        "        if \"assist by\" in text.lower() and \" makes \" in text.lower():\n",
        "            m_a = ASSIST_RE.search(text)\n",
        "            m_f = MAKES_RE.search(text)\n",
        "            if m_a and m_f:\n",
        "                assistant = m_a.group(1).strip()\n",
        "                finisher = m_f.group(1).strip()\n",
        "                if finisher in gsw_players:\n",
        "                    pairs.append((assistant, finisher))\n",
        "    return pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, re, requests\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "links = pd.read_csv(\"/content/gsw10_games_links_2017_18.csv\")\n",
        "\n",
        "GSW_PLAYERS = {\n",
        "    \"Stephen Curry\",\"Klay Thompson\",\"Kevin Durant\",\"Draymond Green\",\"Zaza Pachulia\",\n",
        "    \"Andre Iguodala\",\"Shaun Livingston\",\"David West\",\"JaVale McGee\",\"Jordan Bell\",\n",
        "    \"Kevon Looney\",\"Omri Casspi\",\"Nick Young\",\"Patrick McCaw\",\"Quinn Cook\"\n",
        "}\n",
        "\n",
        "def box_to_pbp_bbref(bbr_box_url: str) -> str:\n",
        "    return bbr_box_url.replace(\"/boxscores/\",\"/boxscores/pbp/\")\n",
        "\n",
        "def game_to_pbp_espn(espn_game_url: str) -> str:\n",
        "    gid = espn_game_url.split(\"gameId/\")[-1].split(\"/\")[0]\n",
        "    return f\"https://www.espn.com/nba/playbyplay/_/gameId/{gid}\"\n",
        "\n",
        "def make_session():\n",
        "    s = requests.Session()\n",
        "    retries = Retry(\n",
        "        total=5, connect=5, read=5,\n",
        "        backoff_factor=1.5,\n",
        "        status_forcelist=[429, 500, 502, 503, 504],\n",
        "        allowed_methods=[\"GET\"]\n",
        "    )\n",
        "    s.headers.update({\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) Colab scraper (academic)\"})\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "session = make_session()\n",
        "\n",
        "ASSIST_BY = re.compile(r\"\\(assist by ([^)]+)\\)\", re.IGNORECASE)\n",
        "MAKES    = re.compile(r\"^(.*?)\\s+makes\\s+\", re.IGNORECASE)\n",
        "\n",
        "def parse_bbref_pairs(html: str, gsw_players: set):\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    desc_tds = soup.find_all(\"td\")\n",
        "    pairs = []\n",
        "    for td in desc_tds:\n",
        "        text = \" \".join(td.get_text(\" \").split())\n",
        "        if \"assist by\" in text.lower() and \" makes \" in text.lower():\n",
        "            m_a = ASSIST_BY.search(text)\n",
        "            m_f = MAKES.search(text)\n",
        "            if m_a and m_f:\n",
        "                assistant = m_a.group(1).strip()\n",
        "                finisher  = m_f.group(1).strip()\n",
        "                if finisher in gsw_players:\n",
        "                    pairs.append((assistant, finisher))\n",
        "    return pairs\n",
        "\n",
        "ESP_ASSIST_BY   = re.compile(r\"\\(assist by ([^)]+)\\)\", re.IGNORECASE)\n",
        "ESP_NAME_ASSIST = re.compile(r\"\\(([^)]+) assists\\)\", re.IGNORECASE)\n",
        "\n",
        "def parse_espn_pairs(html: str, gsw_players: set):\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    pairs = []\n",
        "    for tr in soup.find_all(\"tr\"):\n",
        "        text = \" \".join(tr.get_text(\" \").split())\n",
        "        if \" makes \" in text.lower() and (\"assist\" in text.lower()):\n",
        "            m_f = MAKES.search(text)\n",
        "            finisher = m_f.group(1).strip() if m_f else None\n",
        "            m1 = ESP_ASSIST_BY.search(text)\n",
        "            m2 = ESP_NAME_ASSIST.search(text) if not m1 else None\n",
        "            assistant = (m1.group(1).strip() if m1 else (m2.group(1).strip() if m2 else None))\n",
        "            if finisher and assistant and finisher in gsw_players:\n",
        "                pairs.append((assistant, finisher))\n",
        "    return pairs\n",
        "\n",
        "all_rows = []\n",
        "failures = []\n",
        "\n",
        "for _, row in links.iterrows():\n",
        "    date = row[\"date\"]; opp = row[\"opp\"]\n",
        "    bbr_box = str(row[\"bbr_box\"]).strip()\n",
        "    espn_game = str(row[\"espn_game\"]).strip()\n",
        "\n",
        "    bbref_pbp = box_to_pbp_bbref(bbr_box) if bbr_box else None\n",
        "    espn_pbp  = game_to_pbp_espn(espn_game) if espn_game else None\n",
        "\n",
        "    counts = Counter()\n",
        "\n",
        "    ok = False\n",
        "    try:\n",
        "        if bbref_pbp:\n",
        "            r = session.get(bbref_pbp, timeout=60)\n",
        "            r.raise_for_status()\n",
        "            pairs = parse_bbref_pairs(r.text, GSW_PLAYERS)\n",
        "            if pairs:\n",
        "                for a,b in pairs: counts[(a,b)] += 1\n",
        "                ok = True\n",
        "    except Exception as e:\n",
        "        print(f\"[BBRef timeout/falha] {date} {opp}: {e}\")\n",
        "\n",
        "    if not ok and espn_pbp:\n",
        "        try:\n",
        "            r = session.get(espn_pbp, timeout=60)\n",
        "            r.raise_for_status()\n",
        "            pairs = parse_espn_pairs(r.text, GSW_PLAYERS)\n",
        "            if pairs:\n",
        "                for a,b in pairs: counts[(a,b)] += 1\n",
        "                ok = True\n",
        "        except Exception as e:\n",
        "            print(f\"[ESPN fallback falhou] {date} {opp}: {e}\")\n",
        "\n",
        "    out_rows = [[date, opp, \"GSW\", a, b, c] for (a,b), c in counts.items()]\n",
        "    df_game = pd.DataFrame(out_rows, columns=[\"date\",\"opponent\",\"team\",\"assistant\",\"finisher\",\"assists\"])\n",
        "    df_game.to_csv(f\"/content/pairs_{date}_{opp}.csv\", index=False)\n",
        "    all_rows.extend(out_rows)\n",
        "\n",
        "    print(f\"[{date} {opp}] pares extraídos: {len(out_rows)}\")\n",
        "    time.sleep(2.0)\n",
        "\n",
        "agg = pd.DataFrame(all_rows, columns=[\"date\",\"opponent\",\"team\",\"assistant\",\"finisher\",\"assists\"])\n",
        "agg.to_csv(\"/content/out_pairs_10games.csv\", index=False)\n",
        "\n",
        "summary = agg.groupby([\"date\",\"opponent\"]).agg(\n",
        "    total_assists=(\"assists\",\"sum\"),\n",
        "    unique_pairs=(\"assists\",\"count\"),\n",
        "    gsw_players=(\"finisher\", lambda s: len(set(s)))\n",
        ").reset_index()\n",
        "summary.to_csv(\"/content/summary_by_game.csv\", index=False)\n",
        "\n",
        "print(\"Concluído. Arquivos gerados:\")\n",
        "print(\"/content/out_pairs_10games.csv  (agregado)\")\n",
        "print(\"/content/summary_by_game.csv    (resumo por jogo)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJF6og-p0Wqi",
        "outputId": "136088cb-abbe-490b-adba-af105810d6ee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2017-10-17 HOU] pares extraídos: 0\n",
            "[2017-10-25 TOR] pares extraídos: 0\n",
            "[2017-11-02 SAS] pares extraídos: 0\n",
            "[2017-11-16 BOS] pares extraídos: 0\n",
            "[2017-12-25 CLE] pares extraídos: 0\n",
            "[2018-01-04 HOU] pares extraídos: 0\n",
            "[2018-01-15 CLE] pares extraídos: 0\n",
            "[2018-01-27 BOS] pares extraídos: 0\n",
            "[2018-02-10 SAS] pares extraídos: 0\n",
            "[2018-02-14 POR] pares extraídos: 0\n",
            "Concluído. Arquivos gerados:\n",
            "/content/out_pairs_10games.csv  (agregado)\n",
            "/content/summary_by_game.csv    (resumo por jogo)\n"
          ]
        }
      ]
    }
  ]
}